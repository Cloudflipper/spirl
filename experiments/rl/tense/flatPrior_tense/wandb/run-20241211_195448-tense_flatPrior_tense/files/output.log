exp_dir:	./experiments
conf_path:	spirl/configs/rl/tense/conf.py
general:
	seed:	0
	agent:	<class 'spirl.rl.agents.prior_sac_agent.ActionPriorSACAgent'>
	environment:	<class 'tensegrity_env.tensegrity_env.envs.tensegrity_env.tensegrity_env'>
	data_dir:	.
	num_epochs:	20
	max_rollout_len:	280
	n_steps_per_epoch:	500
	n_warmup_steps:	500.0
agent:
	policy:	<class 'spirl.rl.policies.prior_policies.LearnedPriorAugmentedPIPolicy'>
	policy_params:
		action_dim:	6
		input_dim:	39
		n_layers:	5
		nz_mid:	256
		max_action_range:	1.0
		prior_model:	<class 'spirl.models.bc_mdl.BCMdl'>
		prior_model_params:
			state_dim:	39
			action_dim:	6
		prior_model_checkpoint:	./experiments/skill_prior_learning/tense
	critic:	<class 'spirl.rl.components.critic.MLPCritic'>
	critic_params:
		action_dim:	6
		input_dim:	39
		output_dim:	1
		n_layers:	2
		nz_mid:	256
		action_input:	True
	replay:	<class 'spirl.rl.components.replay_buffer.UniformReplayBuffer'>
	replay_params:
		capacity:	100000.0
		dump_replay:	False
	clip_q_target:	False
	batch_size:	256
	log_video_caption:	True
	td_schedule_params:
		p:	1.0
	device:	cpu
	env_params:
data:
	dataset_spec:
		dataset_class:	<class 'spirl.data.tense.kitchen_data_loader.D4RLSequenceSplitDataset'>
		n_actions:	6
		state_dim:	39
		env_name:	tensegrity_env-v0
		res:	128
		crop_rand_subseq:	True
		max_seq_len:	280
env:
	reward_norm:	1.0
	device:	cpu
	seed:	0
sampler:
ckpt_path:	None
notes:	non-hierarchical RL experiments in tensegrity env
mpi:
	rank:	0
	is_chef:	True
	num_workers:	1
/home/zzz/spirl-master/spirl/modules/layers.py:12: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.
  nn.init.xavier_normal(m.weight.data)
Loading from: ./experiments/skill_prior_learning/tense/weights
=> loading checkpoint './experiments/skill_prior_learning/tense/weights/weights_ep19.pth'
=> loaded checkpoint './experiments/skill_prior_learning/tense/weights/weights_ep19.pth' (epoch 19)
Loading from: ./experiments/skill_prior_learning/tense/weights
=> loading checkpoint './experiments/skill_prior_learning/tense/weights/weights_ep19.pth'
=> loaded checkpoint './experiments/skill_prior_learning/tense/weights/weights_ep19.pth' (epoch 19)
Warmup data collection for 500.0 steps...
/home/zzz/spirl-master/spirl/rl/components/replay_buffer.py:63: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray
  print(key,np.array(example_batch[key]).shape)
observation (500,)
Example Element: (array([-0.018833  ,  0.76160421,  0.55946394,  0.32650337, -0.14239802,
        0.59881552,  0.75234491,  0.23477629, -0.21127728,  0.77378301,
        0.58828759,  0.10266194, -0.00263301,  0.01462624, -0.04528936,
        0.23065031, -0.21550967,  0.10321503, -0.04395373,  0.02805681,
       -0.09476502, -0.08554174, -0.1340997 ,  0.04658854,  0.01130975,
        0.00817633, -0.11343735,  0.13493033,  0.09667476, -0.04552017,
        0.3368821 ,  0.33910543,  0.31890336,  0.34087689,  0.33463521,
        0.31752689,  1.24289907,  1.24283717,  1.25039958]), {})
done (500,)
Example Element: False
reward (500,)
Example Element: 0.08398604608627525
action (500, 6)
Example Element: [-0.39452744 -0.13991053 -0.3238554  -0.20963722 -0.03581854 -0.08134316]
observation_next (500, 39)
Example Element: [-0.0210213   0.76108846  0.56090518  0.32509667 -0.14256528  0.59983743
  0.75198089  0.23322722 -0.20966771  0.77325202  0.58963003  0.10226041
 -0.01011067  0.02293969 -0.04595836  0.19304957 -0.21936238  0.40030648
 -0.05004451  0.02417426 -0.07819767 -0.05514031 -0.12876084  0.13906916
  0.00668094 -0.00519201 -0.08551973  0.12375678  0.09898537  0.26179872
  0.33248872  0.33510598  0.31448168  0.33685095  0.33072246  0.31348351
  1.24462941  1.24451973  1.25210663]
/home/zzz/miniconda3/envs/mujoco/lib/python3.8/site-packages/numpy/core/_asarray.py:136: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray
  return array(a, dtype, copy=False, order=order, subok=True)
Traceback (most recent call last):
  File "spirl/rl/train.py", line 327, in <module>
    RLTrainer(args=get_args())
  File "spirl/rl/train.py", line 77, in __init__
    self.train(start_epoch)
  File "spirl/rl/train.py", line 105, in train
    self.warmup()
  File "spirl/rl/train.py", line 203, in warmup
    self.agent.add_experience(warmup_experience_batch)
  File "/home/zzz/spirl-master/spirl/rl/agents/ac_agent.py", line 187, in add_experience
    self.replay_buffer.append(experience_batch)
  File "/home/zzz/spirl-master/spirl/rl/components/replay_buffer.py", line 38, in append
    self._replay_buffer[key][idxs] = np.stack(experience_batch[key])
  File "<__array_function__ internals>", line 5, in stack
  File "/home/zzz/miniconda3/envs/mujoco/lib/python3.8/site-packages/numpy/core/shape_base.py", line 427, in stack
    raise ValueError('all input arrays must have the same shape')
ValueError: all input arrays must have the same shape
