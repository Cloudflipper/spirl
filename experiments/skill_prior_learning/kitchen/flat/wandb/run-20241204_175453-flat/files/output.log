/home/zzz/spirl-master/spirl/modules/layers.py:12: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.
  nn.init.xavier_normal(m.weight.data)
Reading configurations for Franka
[40m[97mInitializing Franka sim[0m
len train dataset 67790
Reading configurations for Franka
[40m[97mInitializing Franka sim[0m
len val dataset 160
Running Testing

Test set: Average loss: 9.0904 in 0.55s

starting epoch  0
{'states': tensor([[[ 0.0647, -1.7657,  1.8882,  ...,  0.0000,  0.0000, -0.0600],
         [ 0.0443, -1.7572,  1.9057,  ...,  0.0000,  0.0000, -0.0600]],

        [[-0.3289, -1.7596,  1.9326,  ...,  0.0000,  0.0000, -0.0600],
         [-0.2707, -1.7645,  1.9534,  ...,  0.0000,  0.0000, -0.0600]],

        [[-1.1573, -1.5634,  1.0597,  ...,  0.0000,  0.0000, -0.0600],
         [-1.2014, -1.5103,  1.0498,  ...,  0.0000,  0.0000, -0.0600]],

        ...,

        [[-1.8136, -1.4122,  1.0340,  ...,  0.0000,  0.0000, -0.0600],
         [-1.8524, -1.3947,  1.0454,  ...,  0.0000,  0.0000, -0.0600]],

        [[-0.3314, -1.7621,  2.0116,  ...,  0.0000,  0.0000, -0.0600],
         [-0.3630, -1.7624,  2.0251,  ...,  0.0000,  0.0000, -0.0600]],

        [[-1.7536, -1.1052,  1.2988,  ...,  0.0000,  0.0000, -0.0600],
         [-1.7831, -1.0743,  1.3024,  ...,  0.0000,  0.0000, -0.0600]]]), 'actions': tensor([[[-0.1925,  0.0175, -0.0814,  ...,  0.1077, -0.0078, -0.0060]],

        [[ 0.7963,  0.1758,  0.0495,  ..., -0.4209,  0.0060,  0.0579]],

        [[-0.6289,  0.6883, -0.2571,  ...,  0.1621, -0.0165,  0.0110]],

        ...,

        [[-0.5498,  0.4055, -0.0661,  ...,  0.1353, -0.1380, -0.1137]],

        [[-0.3782, -0.0202, -0.0660,  ...,  0.2045,  0.0209, -0.0464]],

        [[-0.4464,  0.5960, -0.1088,  ...,  0.4199, -0.1789, -0.2054]]]), 'pad_mask': tensor([[1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.]], dtype=torch.float64)}
