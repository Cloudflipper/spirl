/home/zzz/spirl-master/spirl/modules/layers.py:12: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.
  nn.init.xavier_normal(m.weight.data)
Reading configurations for Franka
[40m[97mInitializing Franka sim[0m
len train dataset 67790
Reading configurations for Franka
[40m[97mInitializing Franka sim[0m
len val dataset 160
Running Testing

Test set: Average loss: 9.1267 in 0.35s

starting epoch  0
{'states': tensor([[[-1.0119, -1.7719,  1.8577,  ...,  0.0000,  0.0000, -0.0600],
         [-0.9631, -1.7635,  1.8797,  ...,  0.0000,  0.0000, -0.0600]],

        [[-0.9082, -1.7026,  1.3413,  ...,  0.0000,  0.0000, -0.0600],
         [-0.8723, -1.7127,  1.3291,  ...,  0.0000,  0.0000, -0.0600]],

        [[-1.2831, -1.4545,  1.3749,  ...,  0.0000,  0.0000, -0.0600],
         [-1.2734, -1.4674,  1.3898,  ...,  0.0000,  0.0000, -0.0600]],

        ...,

        [[-1.6923, -1.2345,  1.0436,  ...,  0.0000,  0.0000, -0.0600],
         [-1.7435, -1.2129,  1.0680,  ...,  0.0000,  0.0000, -0.0600]],

        [[-1.4045, -1.5596,  1.4848,  ...,  0.0000,  0.0000, -0.0600],
         [-1.3905, -1.5379,  1.4988,  ...,  0.0000,  0.0000, -0.0600]],

        [[-1.3620, -1.7732,  1.9188,  ...,  0.0000,  0.0000, -0.0600],
         [-1.3809, -1.7711,  1.9112,  ...,  0.0000,  0.0000, -0.0600]]]), 'actions': tensor([[[ 0.6989,  0.0812, -0.1570,  ..., -0.2906, -0.2834, -0.0142]],

        [[ 0.4162,  0.0599, -0.2447,  ..., -0.9990, -0.0293, -0.0449]],

        [[ 0.2692,  0.1026, -0.1047,  ...,  0.0745, -0.2490, -0.0531]],

        ...,

        [[-0.6744,  0.4709,  0.0790,  ...,  0.2024, -0.1427, -0.0987]],

        [[-0.1225,  0.4926,  0.1660,  ...,  0.9771, -0.2237, -0.0792]],

        [[-0.2393,  0.0490, -0.2654,  ...,  0.2147, -0.0861, -0.1452]]]), 'pad_mask': tensor([[1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.],
        [1., 1.]], dtype=torch.float64)}
